{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sHOCjLrOcMYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxc621NDuDaC"
      },
      "outputs": [],
      "source": [
        "# 改成你放代码的路径\n",
        "ROOT_PATH = '/content/drive/MyDrive/MSBD5001-Final/sp500'\n",
        "\n",
        "os.chdir(ROOT_PATH)\n",
        "os.listdir(ROOT_PATH)\n",
        "\n",
        "# 读取sp500folder中存在的所有文件夹\n",
        "dirs = os.listdir()\n",
        "\n",
        "for stock in dirs:\n",
        "  # read data\n",
        "  path = os.path.join(ROOT_PATH, stock)\n",
        "  files = os.listdir(path)\n",
        "  \n",
        "  if stock+'_OHLC_1d.csv' not in files or stock+'_info.csv' not in files:\n",
        "    # 如果没有OHLC文件，说明股票不存在，跳过\n",
        "    continue\n",
        "\n",
        "  df = pd.read_csv(path+'/'+stock+'_OHLC_1d.csv')\n",
        "  info = pd.read_csv(path+'/'+stock+'_info.csv')  # info\n",
        "\n",
        "  # drop rows with 'NAN' in month data\n",
        "  df = df.dropna(axis=0,how='any')\n",
        "  df = df.drop(columns=['Date'])\n",
        "  # df['Stock'] = stock\n",
        "  break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面的代码基于上面获取数据集得到的dataframe，每次训练并预测得到一支股票的结果（基于该股票的dataframe）。可以根据需求进行修改。（但是别改git仓库里的）"
      ],
      "metadata": {
        "id": "YCm9jt8hAMIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM"
      ],
      "metadata": {
        "id": "78oRsasoxnOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "写代码的时候懒得切汉语輸入法写注释了就用英文写的，有不懂的地方记得问我\n",
        "下面的代码是分切了测试集训练集的，实际做quant因子的时候，无需分割，把下面training_data_len直接就是len(dataset)即可.\n",
        "深度模型的多步预测尽量使用多个模型分别预测。\n",
        "get_train(1,train_data)处可以多得到适应于每一个steps的测试数据，其中第一个参数为后续要预测的steps数。\n",
        "同时，每增加一个step（默认为3），则需要增加一个model，在train部分中可以添加新的model，已经使用了工厂设计模式，直接得到新的model并投入新的测试数据即可进行训练。\n",
        "prediction部分示例了训练好后的模型如何使用，你们可以根据需要进行使用。    \n",
        "示例使用如下。"
      ],
      "metadata": {
        "id": "QyHl9pC_2b1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data prepare"
      ],
      "metadata": {
        "id": "3ys5ywMF4D3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataframe to a numpy array\n",
        "# df this the dataframe on one stock\n",
        "dataset = df.values\n",
        "# Get the number of rows to train the model on\n",
        "training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
        "\n",
        "# Scale the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "scaled_data[0,1]\n",
        "\n",
        "# Create the training data set \n",
        "# Create the scaled training data set\n",
        "train_data = scaled_data[0:int(training_data_len), :]\n",
        "\n",
        "# Split the data into x_train and y_train data sets\n",
        "# get more data for multisteps prediction\n",
        "def get_train(steps,train_data):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(60, len(train_data)-steps+1):\n",
        "        x_train.append(train_data[i-60:i])\n",
        "        y_train.append(train_data[i+steps-1])\n",
        "        if i<= 61:\n",
        "            print(x_train)\n",
        "            print(y_train)\n",
        "            print()\n",
        "\n",
        "    # Convert the x_train and y_train to numpy arrays \n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "    # Reshape the data\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
        "    return x_train, y_train\n",
        "# x_train_1 means train data for step ==1\n",
        "x_train_1,y_train_1 = get_train(1,train_data)\n",
        "x_train_2,y_train_2 = get_train(2,train_data)\n",
        "x_train_3,y_train_3 = get_train(3,train_data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k8MI0MPTchHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###train"
      ],
      "metadata": {
        "id": "9Vreqe6R0nuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "# Train\n",
        "# Build the LSTM model\n",
        "def model_factory(x_train,y_train):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], x_train.shape[2])))\n",
        "    model.add(LSTM(64, return_sequences=False))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(y_train.shape[1]))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_1 = model_factory(x_train_1,y_train_1)\n",
        "model_2 = model_factory(x_train_2,y_train_2)\n",
        "model_3 = model_factory(x_train_3,y_train_3)\n",
        "\n",
        "# Train the model\n",
        "model_1.fit(x_train_1, y_train_1, batch_size=5, epochs=15)\n",
        "model_2.fit(x_train_2, y_train_2, batch_size=5, epochs=15)\n",
        "model_3.fit(x_train_3, y_train_3, batch_size=5, epochs=15)"
      ],
      "metadata": {
        "id": "TWJydKfkx2N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###prediction"
      ],
      "metadata": {
        "id": "W7JjxUMp0rjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use last data to predict next 3 steps\n",
        "test_data = scaled_data[training_data_len - 60: , :]\n",
        "x_test = []\n",
        "y_test = dataset[training_data_len:, :]\n",
        "for i in range(60, 61):\n",
        "    x_test.append(test_data[i-60:i])\n",
        "    \n",
        "# Convert the data to a numpy array\n",
        "x_test_np = np.array(x_test)\n",
        "\n",
        "# Reshape the data\n",
        "x_test_np = np.reshape(x_test_np, (x_test_np.shape[0], x_test_np.shape[1], x_test_np.shape[2] ))\n",
        "predictions1 = model_1.predict(x_test_np)\n",
        "predictions1 = scaler.inverse_transform(predictions1)\n",
        "predictions2 = model_2.predict(x_test_np)\n",
        "predictions2 = scaler.inverse_transform(predictions2)\n",
        "predictions3 = model_3.predict(x_test_np)\n",
        "predictions3 = scaler.inverse_transform(predictions3)\n",
        "print(predictions1[0])\n",
        "print(y_test[0])\n",
        "print(\"------------------------------\")\n",
        "print(predictions2[0])\n",
        "print(y_test[1])\n",
        "print(\"------------------------------\")\n",
        "print(predictions3[0])\n",
        "print(y_test[2])"
      ],
      "metadata": {
        "id": "_l2fowJ4cmIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40c59bd-aece-4268-eb6c-600abd5bd3b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 656ms/step\n",
            "1/1 [==============================] - 1s 647ms/step\n",
            "[3.5947119e+02 3.6796805e+02 3.5189447e+02 3.6208990e+02 5.5244855e+06]\n",
            "[3.55019989e+02 3.72799988e+02 3.53510010e+02 3.62989990e+02\n",
            " 5.34350000e+06]\n",
            "------------------------------\n",
            "[3.6342960e+02 3.6876837e+02 3.5335291e+02 3.6150275e+02 5.2016930e+06]\n",
            "[3.60790009e+02 3.72549988e+02 3.58220001e+02 3.65329987e+02\n",
            " 3.16820000e+06]\n",
            "------------------------------\n",
            "[3.5972644e+02 3.6505960e+02 3.4779260e+02 3.5513913e+02 4.8447050e+06]\n",
            "[3.71709991e+02 3.78179993e+02 3.67470001e+02 3.76640015e+02\n",
            " 2.53430000e+06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ARIMA"
      ],
      "metadata": {
        "id": "SPIHADvb1szW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在这个部分我直接定义了一个class 方便你们调用使用,做了slice操作只是为了测试训练效果，实际使用时应该直接使用  **df['Close']**  整个丢进去。\n",
        "示例使用如下。"
      ],
      "metadata": {
        "id": "qqH5ETiB5gLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "\n",
        "class ARIMA_class:\n",
        "    def __init__(self, train_x, time_window=5):\n",
        "        # input args train_x-list\n",
        "        self.model = None\n",
        "        self.model_fit = None\n",
        "        self.train_x = train_x\n",
        "        self.time_window = time_window\n",
        "        self.arima_pred = []\n",
        "\n",
        "    def fit(self):\n",
        "        # 阶数等于windwos大小，即与历史前多少天数据相关\n",
        "        self.model = ARIMA(self.train_x, order=(self.time_window, 1, self.time_window)) \n",
        "        self.model_fit = self.model.fit()\n",
        "\n",
        "    def predict(self, valid_len):\n",
        "        # return type pandas.dataframe\n",
        "        forecast = self.model_fit.forecast(valid_len)  \n",
        "        # forecast = forecast.reset_index(drop=True)\n",
        "        return forecast\n"
      ],
      "metadata": {
        "id": "eth2xPmK1BSL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['Close'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uukLjFnv2L5z",
        "outputId": "5ec2b83d-f366-4baf-edaf-a79e0b73f8ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arima_model = ARIMA_class(df['Close'][:-3])"
      ],
      "metadata": {
        "id": "qT4_bqEC1w84"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arima_model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFz-HyXp3Y0T",
        "outputId": "3ae545c2-4d49-445c-b129-03961260d6af"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arima_model.predict(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DoBE_a63eey",
        "outputId": "ca254539-472e-4ff2-89ad-98e049309275"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1444    273.864872\n",
              "1445    274.747956\n",
              "1446    273.744325\n",
              "Name: predicted_mean, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close'][-3:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8KwUf8g3mo2",
        "outputId": "5aae58fa-bddf-41d4-f3ce-3c30c0e529da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1444    281.399994\n",
              "1445    278.250000\n",
              "1446    275.200012\n",
              "Name: Close, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VAR"
      ],
      "metadata": {
        "id": "odd_1K1i5dYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "该部分默认做了一阶差分.\n",
        "同时，该模型严重依赖于过去时间的股价，导致结果严重受影响于当前股价，私以为可以用该模型预测涨跌走势但是不太适合预测价格本身。\n",
        "var_class中的predict函数的第一个参数为用于预测的数据集，一般用倒数var_model.model.k_ar个，第二个参数为要预测的天数。\n",
        "示例使用如下。"
      ],
      "metadata": {
        "id": "3M0hUBP455gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "class VAR_class:\n",
        "    def __init__(self, train_x, time_window=5):\n",
        "        self.train_x_diff = None\n",
        "        self.model = None\n",
        "        self.train_x = train_x\n",
        "        self.time_window = time_window\n",
        "    def diff(self):\n",
        "        self.train_x_diff = self.train_x.diff()\n",
        "        self.train_x_diff = self.train_x_diff.fillna(method=\"bfill\")\n",
        "\n",
        "    def inverse_diff(self,var_pred, second_diff=False):\n",
        "        df_fc = var_pred.copy()\n",
        "        columns = self.train_x.columns\n",
        "        for col in columns:\n",
        "            # # Roll back 2nd Diff\n",
        "            # if second_diff:\n",
        "            #     df_fc[str(col)+'_1d'] = (self.train_x[col].iloc[-1]-self.train_x[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n",
        "            df_fc[str(col) + '_pred'] = self.train_x[col].iloc[0] + df_fc[str(col) + '_pred'].cumsum()\n",
        "        var_pred = df_fc\n",
        "        return var_pred\n",
        "\n",
        "    def fit(self,diff_times=1):\n",
        "        for i in range(diff_times):\n",
        "            self.diff()\n",
        "        self.model = VAR(endog= self.train_x_diff)\n",
        "        self.model = self.model.fit(maxlags=self.time_window)\n",
        "\n",
        "    def predict(self,test_x,valid_len):\n",
        "        test_x = test_x.diff()\n",
        "        test_x = test_x.fillna(method=\"bfill\")\n",
        "        var_pred = self.model.forecast(test_x.values, steps=valid_len)\n",
        "        var_pred = pd.DataFrame(var_pred, columns=self.train_x.columns + '_pred')\n",
        "        var_pred = self.inverse_diff(var_pred)\n",
        "        return  var_pred['Close_pred']"
      ],
      "metadata": {
        "id": "qSFRirzP34TI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_model = VAR_class(df[:-3])\n"
      ],
      "metadata": {
        "id": "wRrQCqrh6LCE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_model.fit()"
      ],
      "metadata": {
        "id": "8KgOtOQx6hFq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag_order = var_model.model.k_ar\n",
        "var_model.predict(df[-lag_order:],3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8wqpIo6gW4",
        "outputId": "228a2f77-b153-454c-bece-9c33413db65b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    102.329694\n",
              "1    102.770830\n",
              "2    104.314593\n",
              "Name: Close_pred, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[-3:]['Close']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNmVRi44-rdI",
        "outputId": "74c09032-e871-4264-c552-0890af9a7915"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1444    281.399994\n",
              "1445    278.250000\n",
              "1446    275.200012\n",
              "Name: Close, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wep9Uz8X-u-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}